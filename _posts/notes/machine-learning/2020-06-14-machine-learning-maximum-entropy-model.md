---
layout: post
title:  "最大熵模型"
date:   2020-06-14 15:20:17 +0800
categories: notes machine-learning
tags: machine-learning 最大熵模型
excerpt: "Maximum Entropy Model"
---

最大熵模型是由最大熵原理所推导的模型。

## 最大熵原理

最大熵原理认为，学习概率模型时，在所有可能的概率模型（分布）中，熵最大的模型是最好的模型。即在满足约束条件的模型集合中选择熵最大的模型。

假设离散随机变量X的概率分布为P(X)，则其熵为：

$$H(P)=-\sum_xP(x)\log P(x)$$

且满足不等式$0\le H(P)\le\log\mid X\mid$，其中\|X\|为X的取值个数，当且仅当X的分布是均匀分布，右边对等号成立，也就是说当X服从均匀分布时熵最大。

最大熵原理将熵最大化表示逼近等可能性。

假设随机变量X有5个取值{A,B,C,D,E}，估计取各值概率P(A)，P(B)，P(C)，P(D)，P(E)。

已知只有5个值ABCDE，所以得到：

$$P(A)+P(B)+P(C)+P(D)+P(E)=1$$

满足该条件的概率为无穷多个，一般认为这个分布中取各值的概率为相等的：

$$P(A)=P(B)=P(C)=P(D)=P(E)=\frac{1}{5}$$

若又有一些概率值约束条件：

$$P(A)+P(B)=\frac{3}{10}$$

满足这两个约束条件的概率分布仍有无穷个。一般认为AB为等概率的，CDE为等概率的，所以：

$$P(A)=P(B)=\frac{3}{20}$$

$$P(A)+P(C)=\frac{7}{30}$$

若是有更多的条件也是按如此进行估计，这种学习方法正是遵循了最大熵原理。即如果未知，就认为它们分布都是均匀的，都是平均分配的。

&emsp;

## 最大熵模型定义

假设分类模型为一个条件概率分布P(Y\|X)，$X\in\mathscr X\in R^n$表示输入，$Y\in \mathscr Y$表示输出，$\mathscr X，\mathscr Y$分别为输入输出的集合。表示给定输入X，以条件概率P(Y\|X)输出Y。

给定训练数据集$T=\lbrace(x_1,y_1),(x_2,y_2)\ldots (x_N,y_N)\rbrace$。

联合分布P(X,Y)的经验分布：

$$\hat P(X=x,Y=y)=\frac{\sum_{i=1}^NI(X_i=x,Y_i=y)}{N}$$

边缘分布P(X)的经验分布：

$$\hat P(X=x)=\frac{\sum_{i=1}^NI(X_i=x)}{N}$$

其中N为训练样本容量。

用特征函数f(x,y)描述输入x和输出y之间的某一个事实，定义为：

$$f(x,y)=
\begin{cases}
1,&x,y满足某一事实\\
0,&否则\\
\end{cases}
$$

其为一个二值函数，当真值就为1，当假值就为0。

特征函数f(x,y)关于经验分布$\hat P(X,Y)$的期望值，用$E_{\hat P}(f)$表示：

$$$$
