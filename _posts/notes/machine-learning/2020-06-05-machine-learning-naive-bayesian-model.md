---
layout: post
title:  "朴素贝叶斯"
date:   2020-06-05 15:21:46 +0800
categories: notes machine-learning
tags: machine-learning 朴素贝叶斯 NBM
excerpt: "Naive Bayesian Model"
---

最为广泛的两种分类模型是决策树模型（Decision Tree Model）和朴素贝叶斯模型（Naive Bayesian Model，NBM）。和决策树模型相比，朴素贝叶斯分类器（Naive Bayes Classifier或NBC）发源于古典数学理论，有着坚实的数学基础，以及稳定的分类效率。同时，NBC模型所需估计的参数很少，对缺失数据不太敏感，算法也比较简单。理论上，NBC模型与其他分类方法相比具有最小的误差率。但是实际上并非总是如此，这是因为NBC模型假设属性之间相互独立，这个假设在实际应用中往往是不成立的，这给NBC模型的正确分类带来了一定影响。

{% raw %}

朴素贝叶斯是基于贝叶斯定理（概论统计学中学过，公式为$P(B_i\mid A)={{P(B_i)P(A\mid B_i)}\over{\sum_{j=1}^n}P(B_j)P(A\mid B_j)}$）与特征条件独立假设（即假设特征向量的每个变量都是相互独立的）。

{% endraw %}

对于给定的训练数据集，基于特征条件独立假设学习输入输出的联合概率分布；然后得到这个模型，基于这个模型，对给定的输入x，利用贝叶斯定义求出后验概率最大的输出y。

## 朴素贝叶斯法的学习

### &emsp;参数定义

设输入空间$\mathscr X \in R^n$为n维向量的集合，输出空间维类标记集合$\mathscr Y=\{c_1,c_2\ldots c_K\}$。输入x属于$\mathscr X$，而输出类标记y为$\mathscr Y$。X为$\mathscr X$上的随机向量，Y为$\mathscr Y$上的随机变量，而P(X,Y)为XY的联合概率分布。

训练数据集$T=\{(x_1,y_1),(x_2,y_2)\ldots (x_N,y_N)\}$由P(X,Y)独立同分布产生。

### &emsp;联合概率分布计算

朴素贝叶斯首先学习先验概率分布$P(Y=c_k),k=1,2\ldots K$

先验概率（prior probability）是指根据以往经验和分析得到的概率，如全概率公式，它往往作为"由因求果"问题中的"因"出现的概率。这里的先验概率是指Y为k个类的每个可能性的概率。

然后计算条件概率分布$P(X=x\mid Y=c_k)=P({X^{(1)}=x^{(1)},\ldots ,X^{(n)}=x^{(n)}\mid Y=c_k}),k=1,2\ldots K$

即当实例X为x特征向量时类别为$c_k$的概率是类别为$c_k$时对应的特征向量等于特定值的概率。

所以就可以根据这两个数据得到联合概率分布P(X,Y)，即X的条件下类型是Y的概率分布。

要得到条件概率分布$P(X=x\mid Y=c_k)$要先求出对应的条件概率，参数是指数级别的，如果参数过多，基本上不可能求出来每一个具体的概率。（因为假设$x^{(j)}$可取值有$S_j$个，j=1,2...n，Y的可取值为K个，那么可取参数对为$K\prod_{j=1}^n S_j$）

### &emsp;条件独立性假设

朴素贝叶斯法对条件概率分布做出了条件独立性的假设，因为这是个约束较强的假设（因为基本很少条件会是完全独立的，就比如身高体重），所以就叫朴素贝叶斯法（假设后模型相对很简单）。

独立性假设是：$P(X=x\mid Y=c_k)=P({X^{(1)}=x^{(1)},\ldots ,X^{(n)}=x^{(n)}\mid Y=c_k})=\prod_{j=1}^nP(X^{(j)}=x^{(j)}\mid Y=c_k)$
