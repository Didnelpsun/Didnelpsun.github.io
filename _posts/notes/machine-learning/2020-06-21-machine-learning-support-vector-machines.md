---
layout: post
title:  "支持向量机"
date:   2020-06-21 18:10:22 +0800
categories: notes machine-learning
tags: 机器学习 基础 machine-learning 支持向量机
excerpt: "Support Vector Machines"
---

支持向量机是一种二类分类模型，基本模型是定义在特征空间上的间隔最大的线性分类器。支持向量机的学习策略就是间隔最大化，可形式化为一个求解凸二次规划的问题，也等价于正则化的合页损失函数最小问题。所以支持向量机的学习算法是求解凸二次规划的最优化算法。

支持向量机学习方法包含构建由简到繁的模型：线性可分支持向量机、线性支持向量机与非线性支持向量机。当训练数据线性可分，通过硬间隔最大化学习一个线性的分类器，就是线性可分支持向量机，又称为硬间隔支持向量机；当训练数据近似线性可分时，通过软间隔最大化，也学习一个线性的分类器，即线性支持向量机，又称为软间隔支持向量机；当训练数据线性不可分时，通过核技巧及软间隔最大化，就是学习非线性支持向量机。

当输入空间为欧式空间或者离散集合、特征空间为希尔伯特空间时，核函数表示将输入从输入空间映射到特征空间得到的特征向量之间的内积。通过使用核函数可以学习非线性支持向量机，等价于隐式地在高维特征空间中学习线性支持向量机。这样的方法称为核技巧。即通过高维函数将非线性变为线性。核方法是比支持向量机更一般的机器学习方法。

## 线性可分支持向量机与硬间隔最大化

### &emsp;线性可分支持向量机

考虑一个二类分类问题。假设输入空间与特征空间为两个不同的空间。输入空间为欧式空间或离散集合，特征空间为欧式空间或希尔伯特空间。线性可分支持向量机，线性支持向量机假设这两个空间的元素一一对应，并将输入空间的输入映射为特征空间中的特征向量。非线性支持向量机利用一个从输入空间到特征空间的非线性映射将输入映射为特征向量。所以输入都由输入空间转换为特征空间，支持向量机的学习是在特征空间上进行的。

特征训练集$T=\lbrace(x_1,y_1),(x_2,y_2)\ldots (x_N,y_N)\rbrace$在一个特征空间上。

其中，$x_i\in \mathscr X=R^n,y_i\in \mathscr Y=\lbrace+1,-1\rbrace,i=1,2\ldots N$。$x_i$为第i个特征向量，也称为实例，$y_i$为$x_i$的类标记。当$y_i=+1$时，称$x_i$为正例；当$y_i=-1$时，称$x_i$为负例。$(x_i,y_i)$称为样本点。再假设训练数据集为线性可分的。

学习的目标为特征空间中找到一个分离超平面，能将实例分到不同的类上。分离超平面对应于方程$\omega\cdot x+b=0$，由法向量ω和截距b，可用(ω,b)来表示。分离超平面将特征空间划分为正类部分和负类部分。法向量指向的一侧为正类和负类。

当训练数据集线性可分时，存在无穷个分离超平面可将两类数据正确分开。感知机利用误分类最小策略，求得分离超平面，不过此时解无穷，线性可分支持向量机利用间隔最大化求出最优分离超平面，解此时唯一。

给定线性可分训练数据集，通过间隔最大化或等价求解相应的凸二次规划问题学习得到的分离超平面为：

$$\omega^\ast\cdot x+b^\ast=0$$

以及相应的分类决策函数：

$$f(x)=sign(\omega^\ast\cdot x+b^\ast=0)$$

就是线性可分支持向量机。

### &emsp;函数间隔和几何间隔


