---
layout: post
title:  "Java知识点"
date:   2022-05-30 18:24:54 +0800
categories: notes java knowledge
tags: Java 高级 知识
excerpt: "Java知识点"
---

Java

## HashMap和HashTable区别

&emsp;|HashMap|HashTable
:-------:|:---------:|:----------:
效率|高|低
线程安全|否|是
允许key和value为null|是|否
遍历方式|迭代器|迭代器、索引

+ 允许key和value为null：HashMap的key只能有一个且必须在0位置，value可以有多个，这是由于直接使用key.hashcode()，而没有向HasMap一样先判断key是否为null，所以key为null时，调用key.hashcode()会出错，所以hashtable中key也不能为null。

使用拉链法实现，即数组Entry+链表，用于存储键值对。

HashMap当链表增长长度大于阈值（默认为8）会准备扩容，首先判断如果当前数组Entry的长度小于64，那么会选择先进行数组扩容；如果大于等于64时，将链表转化为红黑树。当数据减少，红黑树高低于6时，红黑树会重新变为链表。

HashTable默认capacity是11，默认负载因子是0.75。当前表中的Entry数量，如果超过了阈值，就会扩容，即调用rehash方法，重新计算每个键值对的hashCode，判断新的容量是否超过了上限，没超过就新建一个新数组，大小为原数组的2倍+1，将旧数的键值对重新hash添加到新数组中。

## HashMap安全性

方法一:通过Collections.synchronizedMap()返回一个新的Map,这个新的map就是线程安全的.这个要求大家习惯基于接口编程,因为返回的并不是HashMap,而是一个Map的实现。

方法二:重新改写了HashMap,具体的可以查看java.util.concurrent.ConcurrentHashMap.这个方法比方法一有了很大的改进。

方法一特点:
通过Collections.synchronizedMap()来封装所有不安全的HashMap的方法,就连toString, hashCode都进行了封装.封装的关键点有2处1)使用了经典的synchronized来进行互斥,2)使用了代理模式new了一个新的类,这个类同样实现了Map接口.在Hashmap上面,synchronized锁住的是对象,所以第一个申请的得到锁,其他线程将进入阻塞,等待唤醒.优点:代码实现十分简单,一看就懂.缺点:从锁的角度来看;方法一直接使用了锁住方法,基本上是锁住了尽可能大的代码块.性能会比较差。

方法二特点:
重新写了HashMap,比较大的改变有如下几点.使用了新的锁机制,把HashMap进行了拆分,拆分成了多个独立的块,这样在高并发的情况下减少了锁冲突的可能,使用的是NonfairSync.这个特性调用CAS指令来确保原子性与互斥性.当如果多个线程恰好操作到同一个segment上面,那么只会有一个线程得到运行.
优点:需要互斥的代码段比较少;性能会比较好.ConcurrentHashMap把整个Map切分成了多个块,发生锁碰撞的几率大大降低，性能会比较好.缺点:代码繁琐

## ArrayList扩容机制

### add方法进行触发

ArrayList默认无参构造时初始空间JDK8之前为10，JDK8之后大小为0，只有添加数据时才默认扩容为10。此后默认都是原来容量的大概1.5倍，添加原容量大小右移一位（类似除以2）的大小。

### addAll方法进行触发

add方法都是一次只添加一个数据，而addAll方法是一次性添加多个数据。

ArrayList默认无参构造时初始空间JDK8之前为10，JDK8之后大小为0，只有添加数据且集合数据大小不超过10才默认扩容为10。扩容时会取原始容量的一半和添加数据的数量进行比较，选取较大值作为扩容添加的大小。

## ArrayList和LinkedList

ArrayList和LinkedList都实现了List接口，他们有以下的不同点：

ArrayList是基于索引的数据接口，它的底层是数组。它可以以o(1)时间复杂度对元素进行随机访问。与此对应，LinkedList是以元素列表的形式存储它的数据，每一个元素都和它的前一个和后一个元素链接在一起，在这种情况下，查找某个元素的时间复杂度是O(n)。

相对于ArrayList，LinkedList的插入，添加，删除操作速度更快，因为当元素被添加到集合任意位置的时候，不需要像数组那样重新计算大小或者是更新索引。

LinkedList比ArrayList更占内存，因为LinkedList为每一个节点存储了两个引用，一个指向前一个元素，一个指向下一个元素。

1. 因为Array是基于索引(index)的数据结构，它使用索引在数组中搜索和读取数据是很快的。Array·获取数据的时间复杂度是O(1),但是要删除数据却是开销很大的，因为这需要重排数组中的所有数据。
2. 相对于ArrayList,LinkedList插入是更快的。因为LinkedList 不像ArrayList一样，不需要改变数组的大小，也不需要在数组装满的时候要将所有的数据重新装入一个新的数组，这是ArrayList最坏的一种情况，时间复杂度是O(n)，而LinkedList 中插入或删除的时间复杂度仅为o(1)。ArrayList 在插入数据时还需要更新索引（除了插入数组的尾部)。
3. 类似于插入数据，删除数据时， LinkedList也优于ArrayList。
4. LinkedList 需要更多的内存，因为 ArrayList的每个索引的位置是实际的数据，而LinkedList中的每个节点中存储的是实际的数据和前后节点的位置(一个LinkedList 实例存储了两个值: Node\<E\> first和Node\<E\> last分别表示链表的其实节点和尾节点，每个Node 实例存储了三个值: (E item,Node next,Node pre)。

什么场景下更适宜使用LinkedList，而不用ArrayList。

1. 你的应用不会随机访问数据。因为如果你需要LinkedList中的第n个元素的时候，你需要从第一个元素顺序数到第n个数据，然后读取数据。
2. 你的应用更多的插入和删除元素，更少的读取数据。因为插入和删除元素不涉及重排数据，所以它要比ArrayList要快。

## 迭代器

用来遍历集合。用迭代器进行遍历时，能否让其他线程修改迭代器内容，这就需要不同的处理策略。

+ fail-fast：即遍历时发现其他线程进行修改就立刻抛出异常。
+ fail-safe：即遍历时发现其他线程进行修改继续执行，牺牲一致性。

其中ArrayList和Vector的迭代器的策略就是fail-fast，如果有其他线程修改迭代器内容，ArrayList会在迭代器的下一轮遍历抛出异常ConcurrentModificationException。

CopyOnWriteArrayList的迭代器的策略就是fail-safe，迭代器遍历时不会抛出异常。

这是因为CopyOnWriteArrayList的add等修改方法默认实现都是将原本的数组赋值一份，将数据修改到新数组中，然后将变量指针指向新数组，旧数组被GC回收，使用读写分离技术，所以迭代器访问和修改时不是一个数组，从而可以互不干扰。

<!-- ## JDK1.8新技术

提供接口默认default实现。提供面向函数式编程lambda表达式。多重注解。 -->

## 抽象类与接口

相同：

+ 不能实例化。
+ 可以通过子类或实现类进行向上转型作为引用类型。
+ 必须全部实现抽象方法。

不同：

&emsp;|抽象类|接口
构造器|是|否
方法类型|抽象、具体|抽象、默认、静态
成员权限|public|无限制
成员变量类型|变量和常量|常量
继承|一个|多个

抽象类用于全局类型概念，接口用于局部特征行为。

## CAS自旋锁

<https://blog.csdn.net/weixin_46253250/article/details/120910050>。

JVM

## 对象组成结构

+ markword：8字节（64位计算机），包括Lock锁信息，HashCode，GC垃圾处理信息。
+ class pointer：4字节，类型指针，指向当前对象的类型。markword+class pointer即为对象头。
+ instance data：实例数据，即对象中的成员变量。
+ padding：对齐，让对象总大小能被8整除。

## 对象定位方式

+ 直接方式：直接指针。指针->堆中对象实例成员->类型数据指针->方法区的类。
+ 间接方式：句柄方式。指针->实例数据指针；类型数据指针->堆中对象实例成员；方法区的类。

数据库

## ACID

+ 原子性由undalog日志来保狂，它记录了需要回滚的日志信息，事务回滚时撤销已经执行成功的sql。
+ 一致性是由其他三大特性保证，程序代码要保证业务上的一致性。
+ 隔离性是由MVCC（多版本并发控制）来保证。
+ 持久性由redolog来保证，mysql修改数据的时候会在redolog中记录一份日志数据，就算数据没有保存成功，只要日志保存成功了，数据仍然不会丢失。

## MVCC

### &emsp;场景

数据库并发场景有三种。分别为：

1. 读读：不存在任何问题。也不需要并发控制。
2. 读写：有线程安全问题，可能会造成事务隔离性问题，可能遇到脏读、幻读、不可主复读。
3. 写写：有线程安全问题，可能存在更新丢失问题。

MVc是一种用来解决读写冲突的天额并发控制，也就足为事务分配单项增长的时间瑟，为每个修改保存一个版本，版本与牛务时间戳关联，读操作只读该务开始前的数据库的快照，所以MVCC可以为数据库解决一下问题：

1. 在并发读写数据库时，可以做到在读操作时不用阳塞写挨作，写操作也不用阻塞读操作，提高了数据库并发读写的性能。
2. 解决脏读、幻读、不可重复读等事务隔离问题，但是不能解决更新丢失问题。

### &emsp;原理

MVCC的实现原理主要依赖于记录中的三个隐藏字段、undolog、read view来实现的。

#### &emsp;&emsp;隐藏字段

每行记录除了我们自己定义的字段外，还有数据库隐式定义的DB_TRX_JD,DB_ROLL_PTR,DB_ROW_ID等字段。

+ DB_TRX_ID：6字节，最近修改事务id，记录创建这条记录或者最后一次修改该记录的事务id。
+ DB_ROLL_PTR：7字节，回滚指针，指向这条记录的上一个版本，用于配合undolog，指向上一个旧版本。
+ DB_Row_JD：6字节，隐藏的主键，如果数据表没有主键，那么innodb会自动生成一个6字节的row_id。

#### &emsp;&emsp;undo_log

表示在进行insert，delete，update操作的时候产生的方使回滚的日志当。

进行insert操作的时候，产生的undolog只在事务回滚的时候需要，并且在事务提交之后可以被立刻丢弃。

当进行update和delete操作的时候，产生的undolog不仅仅在事务回滚的时候需要，在快照读的时候也需要，所以不能随便删除，只有在快照读或事务回滚不涉及该日志时，对应的日志才会被purge线程统一清除（当数据发生更新和删除操作的时侯部只是设置一下老记录的deleted_bit，并不是真正的将过时的记录删除，因为为了节省磁盘空间，innodb有专门的purge线程来清除deleted_bit为true的记录，如果某个记录的celeted_id为true，并且DB_TRX_ID相对于purge线程的read view可见，那么这条记录一定时可以被清除的。

#### &emsp;&emsp;read view

上面的流程如果看明白了，那么要再深入理解下read view的概念。

Read View是事务进行快照读操作的时候生产的读视图，在该事务执行快照读的那一刻，会生成一个数据系统当前的快照，记录并维护系统当前活跃事务的id，事务的id值是递增的。

其实Read View的最大作用足用来做可见性判断的，也就足说当某个事务在执行快读的时候，对该记录创建一个Read View的识图，把它当作条件去判断当前事务能够看到哪个版本的数据，有可能读取到的是最新的数据，也有可能读取的是当前行记录的undolog中某个版本的数据。

Read View遵循的可见性算法主要是将要被修改的数据的最新记录中的DB_TRX_JD（当前事务id）取出来，与系统当前其他活跃事务的id去对比，如果DB_TRX_IDE跟Read View的展性做了比较，不符合可见性，那么就通过DB_ROLL_PTR回滚指针去取出undolog中的DE_TRX_ID做比较，即邃历链表中的DE_TRX_ID，直到找到满足条件的DB_TRX_ID,这个DB_TRX_ID所在的旧记录就是当前事务能看到的最新老版本数据。

具有三个全局属性：

+ trx_list：一个数值列表，用来维护Read View生成时刻系统正活跃的事务ID。
+ up_limit_id：记录trx_list列表中事务ID最小的ID。
+ low_limit_id：Read View生成时刻系统尚未分配的下一个事务ID。

具体的比较规则如下：

1. 首先比较DB_TRX_ID < up_limit_id,如果小于，则当前事务能看到DB_TRX_ID所在的记录，如果大于等于进入下一个判断。
2. 接下来判断DB_TRX_ID >= low_limit_id,如果大于等于则代表DB_TRX_ID所在的记录在Read View生成后才出现的，那么对于当前事务肯定不可见，如果小于，则进入下一步判断。
3. 判断DB_TRX_ID是否在活跃事务中，如果在，则代表在Read View生成时刻，这个事务还是活跃状态，还没有commit，修改的数据，当前事务也是看不到，如果不在，则说明这个事务在Read View生成之前就已经开始commit，那么修改的结果是能够看见的。

RC、RR级别下的InnoDB快照读有什么不同：因为Read view生成时机的不同，从而造成RC、RR级别下快照读的结果的不同。

1. 在RR级别下的某个事务的对某条记录的第一次快照读会创建一个快照即Read View；将当前系统活跃的其他事务记录起来，此后在调用快照读的时候，还是使用的是同一个Read Vview，所以只要当前事务在其他事务提交更新之前使用过快照读，那么之后的快照读使用的都是同一个Read View，所以对之后的修改不可见
2. 在RR级别下，快照读生成Read View时，Read View会记录此时所有其他活动和事务的快照，这些事务的修改对于当前事务都是不可见的，而早于Read View创建的事务所做的修改均是可见
3. 在RC级别下，事务中，每次快照读都会新生成一个快照和Read View，这就是我们在RC级别下的事务中可以看到别的事务提交的更新的原因。
总结:在RC隔离级别下，是每个快照读都会生成并获取最新的Read View，而在RR隔离级别下，则是同一个事务中的第一个快照读才会创建Read View，之后的快照读获取的都是同一个Read View。

## MySQL隔离级别

MySQL定义了四种隔离级别，包括一些具体规则，用于限定事务内外哪些改变是可见的，哪些改变是不可见的。低级别的隔离一般支持更高的并发处理，并且拥有更低的系统开销。

### &emsp;读问题

1. 脏读：是指一个事务读取了未提交事务执行过程中的数据。当一个事务的操作正在多次修改数据，而在事务还未提交的时候，另外一个并发事务来读取了数据，就会导致读取到的数据并非是最终持久化之后的数据，这个数据就是脏读的数据。
2. 不可重复读：不可重复读是指对于数据库中的某个数据，一个事务执行过程中多次查询返回不同查询结果，这就是在事务执行过程中，数据被其他事务提交修改了。不可重复读同脏读的区别在于，脏读是一个事务读取了另一未完成的事务执行过程中的数据，而不可重复读是一个事务执行过程中，另一事务提交并修改了当前事务正在读取的数据。
3. 幻读：是事务非独立执行时发生的一种现象，例如事务T1批量对一个表中某一列列值为1的数据修改为2的变更，但是在这时，事务T2对这张表插入了一条列值为1的数据，并完成提交。此时，如果事务T1查看刚刚完成操作的数据，发现还有一条列值为1的数据没有进行修改，而这条数据其实是T2刚刚提交插入的，这就是幻读。幻读和不可重复读都是读取了另一条已经提交的事务（这点同脏读不同），所不同的是不可重复读查询的都是同一个数据项，而幻读针对的是一批数据整体（比如数据的个数）。

### &emsp;READ UNCOMMITTED读取未提交内容

在这个隔离级别，所有事务都可以"看到"未提交事务的执行结果。在这种级别上，可能会产生很多问题，除非用户真的知道自己在做什么，并有很好的理由选择这样做。本隔离级别很少用于实际应用，因为它的性能也不必其他性能好多少，而别的级别还有其他更多的优点。读取未提交数据-也被称为"脏读"。

### &emsp;READ COMMITTED读取提交内容

大多数数据库系统的默认隔离级别(但是不是MySQL的默认隔离级别)，满足了隔离的早先简单定义:一个事务开始时，只能"看见"已经提交事务所做的改变，一个事务从开始到提交前，所做的任何数据改变都是不可见的，除非已经提交。这种隔离级别也支持所谓的"不可重复读"。这意味着用户运行同一个语句两次，看到的结果是不同的。

### &emsp;REPEATABLE READ可重复读

MySQL数据库默认的隔离级别。该级别解决了READ UNCOMMITED隔离级别导致的问题。它保证同一事务的多个实例在并发读取事务时，会看到同样的数据行。不过，这会导致另外一个棘手问题"幻读"。InnoDB和Falcon存储引擎通过多版本并发控制MVCC机制解决了幻读问题。

### &emsp;SERIALIZABLE可串行化

该级别是最高级别的隔离级。它通过强制事务排序，使之不可能相互冲突，从而解决幻读问题。简而言之，SERIALIZABLE是在每个读的数据行上加锁。在这个级别，可能导致大量的超时Timeout和锁竞争Lock Contention现象，实际应用中很少使用到这个级别，但如果用户的应用为了数据的稳定性，需要强制减少并发的话，也可以选择这种隔离级。

## MySQL索引

### &emsp;索引作用

一般的应用系统，读写比例在10:1左右，而且插入操作和一般的更新操作很少出现性能问题，在生产环境中，我们遇到最多的，也是最容易出问题的，还是一些复杂的查询操作，因此对查询语句的优化显然是重中之重。说起加速查询，就不得不提到索引了。

### &emsp;索引定义

索引在MySQL中也叫是一种"键”，是存储引擎用于快速找到记录的一种数据结构。索引对于良好的性能非常关键，尤其是当表中的数据量越来越大时，索引对于性能的影响愈发重要。

索引优化应该是对查询性能优化最有效的手段了。索引能够轻易将查询性能提高好几个数量级。索引相当于字典的音序表，如果要查某个字，如果不使用音序表，则需要从几百页中逐页去查。

### &emsp;索引原理

索引的目的在于提高查询效率，与我们查阅图书所用的目录是一个道理：先定位到章，然后定位到该章下的一个小节，然后找到页数。相似的例子还有:查字典，查火车车次，飞机航班等。本质都是通过不断地缩小想要获取数据的范围来筛选出最终想要的结果，同时把随机的事件变成顺序的事件，也就是说，有了这种索引机制，我们可以总是用同一种查找方式来锁定数据。

数据库也是一样，但显然要复杂的多，因为不仅面临着等值查询，还有范围查询(>、<、between、in)、模糊查询(like)、并集查询(or)等等。数据库应该选择怎么样的方式来应对所有的问题呢？我们回想字典的例子，能不能把数据分成段，然后分段查询呢?最简单的如果1000条数据，1到100分成第一段，101到200分成第二段，201到300分成第三段..这样查第250条数据，只要找第三段就可以了，一下子去除了90%的无效数据。但如果是1千万的记录呢，分成几段比较好?按照搜索树的模型，其平均复杂度是lgN，具有不错的查询性能。但这里我们忽略了一个关键的问题，复杂度模型是基于每次相同的操作成本来考虑的。而数据库实现比较复杂，一方面数据是保存在磁盘上的，另外一方面为了提高性能，每次又可以把部分数据读入内存来计算，因为我们知道访问磁盘的成本大概是访问内存的十万倍左右，所以简单的搜索树难以满足复杂的应用场景。

### &emsp;索引数据结构

MySQL主要用到两种结构：B+Tree索引和Hash索引。

+ Inodb存储引擎默认是B+Tree索引。
+ Memory存储引擎默认Hash索引。

MySQL中，只有Memory（Memory表只存在内存中，断电会消失，适用于临时表）存储引擎显示支持Hash索引，是Memory表的默认索引类型，尽管Memory表也可以使用B+Tree索引。Hash索引把数据以hash形式组织起来，因此当查找某一条记录的时候，速度非常快。但是因为hash结构，每个键只对应一个值，而且是散列的方式分布。所以它并不支持范围查找和排序等功能。

B+Tree是MySQL使用最频繁的一个索引数据结构，是InnoDB和MyISAM存储引擎模式的索引类型。相对Hash索引，B+Tree在查找单条记录的速度比不上Hash索引，但是因为更适合排序等操作，所以它更受欢迎。毕竟不可能只对数据库进行单条记录的操作。

对比：

+ hash类型的索引：查询单条快，范围查询慢。
+ btree类型的索引: b+树，层数越多，数据量指数级增长（我们就用它，因为InnoDB默认支持它)

## 索引类型

+ 普通索引：允许被索引的数据列包含重复的值。
+ 唯一索引：可以保证数据记录的唯一性。
+ 主键索引：是一种特殊的唯一索引，在一张表中只能定义一个主键索引，主键用于唯一标识一条记录，使用关键字primary key来创建。可以包含多个列，但是不能为空。
+ 联合索引：索引可以覆盖多个数据列。
+ 全文索引：通过建立倒排索引，可以极大的提升检索效率，解决判断字段是否包含的问题，是目前搜索引擎使用的一种关键技术。

优缺点：

+ 索引可以极大地提高数据的查询速度。
+ 通过使用索引，可以在查询的过程中，使用优化隐藏器，提高系统的性能。
+ 但是会降低插入、删除、更新表的速度，因为在执行这些写操作的时候，还要操作索引文件。
+ 索引需要占物理空间，除了数据表占数据空间之外，每一个索引还要占一定的物理空间，如果要简历聚簇索引，那么需要的空间就会更大，如果非聚簇索引很多，一旦聚簇索引改变，那么所有非聚簇索引都会跟着变。

## 索引选择

1. 适合索引的列是出现在where字句中的列，或者连接子句中指定的列，即查询条件。
2. 基数较小的表，索引效果差，没必要创建索引。
3. 在选择索引列的时候，越短越好，可以指定某些列的一部分，没必要用全部字段的值。
4. 不要给表中的每一个字段都创建索引，并不是索引越多越好。
5. 定义有外键的数据列一定要创建索引。
6. 更新频繁的字段不要有索引。
7. 创建索引的列不要过多，可以创建组合索引，但是组合索引的列的个数不建议太多。
8. 大文本、大对象不要创建索引。

## MySQL锁类型

基于锁的属性分类：

+ 共享锁（share lock）：共享锁又称读锁，简称S锁。当一个事务为数据加上读锁之后，其他事务只能对该数据加读锁，而不能对数据加写锁，直到所有的读锁释放之后其他事务才能对其进行加持写锁。共享锁的特性主要是为了支持并发的读取数据，读取数据的时候不支持修改，避免出现重复读的问题。
+ 排他锁（exclusive lock）：排他锁又称写锁，简称X锁。当一个事务为数据加上写锁时，其他请求将不能再为数据加任何锁，直到该锁释放之后，其他事务才能对数据进行加锁。排他锁的目的是在数据修改时候，不允许其他人同时修改，也不允许其他人读取，避免了出现脏数据和脏读的问题。

基于锁的粒度分类：

+ 表锁（table lock）：InnoDB、Myisam使用。表锁是指上锁的时候锁住的是整个表，当下一个事务访问该表的时候，必须等前一个事务释放了锁才能进行对表进行访问。特点：粒度大，加锁简单，容易冲突。
+ 行锁：Innodb使用。行锁是指上锁的时候锁住的是表的某一行或多行记录，其他事务访问同一张表时，只有被锁住的记录不能访问，其他的记录可正常访问，特点：粒度小，加锁比表锁麻烦，不容易冲突，相比表锁支持的并发要高。
+ 页级锁：Innodb使用。页级锁是MysQL中锁定粒度介于行级锁和表级锁中间的一种锁．表级锁速度快，但冲突多，行级冲突少，但速度慢。所以取了折衷的页级，一次锁定相邻的一组记录。特点；开销和加锁时间界于表锁和行锁之间，会出现死锁；锁定粒度界于表锁和行锁之间，并发度一般。
+ 记录锁（Record lock）：记录锁也属于行锁中的一种，只不过记录锁的范围只是表中的某一条记录，记录锁是说事务在加锁后锁住的只是表的某一条记录，加了记录锁之后数据可以避免数据在查询的时候被修改的重复读问题，也避免了在修改的事务未提交前被其他事务读取的脏读问题。
+ 间隙锁：是属于行锁的一种，间隙锁是在事务加锁后其锁住的是表记录的某一个区间，当表的相邻ID之间出现空隙则会形成一个区间，遵循左开右闭原则。范围查询并且查询未命中记录，查询条件必须命中索引、间隙锁只会出现在REPEATABLE_READ（重复读）的事务级别中，解决幻读的问题。
+ 临键锁（Next-Key lock）：也属于行锁的一种，并且它是InnoDB的行锁默认算法，总结来说它就是记录锁和间隙锁的组合，临键锁会把查询出来的记录锁住，同时也会把该范围查询内的所有间隙空间也会锁住，再之它会把相邻的下一个区间也会锁住。

基于锁的状态分类：意向共享锁、意向排它锁。

## MySQL聚簇与非聚簇索引

数据和索引是否放在一起，一起就是聚簇，否则就是非聚簇。

mysql的索引类型跟存储引擎是相关的，innodb存储引擎数据文件跟索引文件全部放在ibd文件中，而
myisam的数据文件放在myd文件中，索引放在myi文件中，其实区分聚簇索引和非聚簇索引非常简单，只要判断数据跟索引是否存储在一起就可以了。

innodb存储引擎在进行数据插入的时候，数据必须要跟索引放在一起，如果有主键就使用主键，没有主键就使用唯一键，没有唯一键就使用6字节的rowid，因此跟数据绑定在一起的就是聚簇索引，而为了避免数据冗余存储，其他的索引的叶子节点中存储的都是聚簇索引的key值，因此innodb中既有聚簇索引也有非聚簇索引，而myisam中只有非聚簇索引。

## 执行计划

在企业的应用场景中，为了知道优化SQL语句的执行，需要查看SQL语句的具体执行过程，以加快SQL语句的执行效率。

可以使用explain+SQL语句来模拟优化器执行SpL查询语句，从而知道mysql是如何处理sql语句的。

[官网地址](https://dev.mysql.com/doc/refman/5.7len/explain-output.html)。

## Myisam何Innodb的区别

InnoDB存储引擎：主要面向OLTP（Online Transaction Processing，在线事务处理）方面的应用，是第一个完整支持ACID事务的存储引擎（BDB第一个支持事务的存储引擎，已经停止开发）。

特点:

1. 支持行锁。
2. 支持外键。
3. 支持自动增加列AUTO_INCREMENT属性，通过自增锁实现。
4. 支持事务。
5. 支持MVCC模式的读写。
6. 读的效率低于Myisam。
7. 写的效率高优于Myisam。
8. 适合频繁修改以及设计到安全性较高的应用
9. 清空整个表的时候，Innodb是一行一行的删除，

Myisam存储引擎：是MySQL官方提供的存储引擎，主要面向OLAP（Online Analytical Processing，在线分析处理）方面的应用。

特点：

1. 独立于操作系统，当建立一个Myisam存储引擎的表时，就会在本地磁盘建立三个文件，例如我建立tb_demo表，那么会生成以下三个文件tb_demo.frm,tb_demo.MYD,tb_demo.MYI。
2. 不支持事务。
3. 支持表锁和全文索引。
4. Myisam存储引擎表由MYD和MYI组成，MYD用来存放数据文件，MYI用来存放索引文件。MySQL数据库只缓存其索引文件，数据文件的缓存交给操作系统本身来完成。
5. MySQL5.0版本开始，Myisam默认支持256T的单表数据。
6. 选择密集型的表，Myisam存储引擎在筛选大量数据时非常迅速，这是他最突出的优点。
7. 读的效率优于InnoDB。
8. 写的效率低于InnoDB。
9. 适合查询以及插入为主的应用
10. 清空整个表的时候，Myisam则会新建表。

## 慢查询

1. 开启慢查询日志，准确定位到哪个SQL语句出现了问题。
2. 分析SQL语句，看看是否load了额外的数据，可能是查询了多余的行并且抛弃掉了，可能是加载了许多结果中并不需要的列，对语句进行分析以及重写。
3. 分析语句的执行计划，然后获得其使用索引的情况，之后修改语句或者修改索引，使得语句可以尽可能的命中索引。
4. 如果对语句的优化已经无法进行，可以考虑表中的数据量是否太大，如果是的话可以进行横向或者纵向的分表。

## 事务特性

事务四大特征:原子性，一致性，隔离性和持久性。

1. 原子性（Atomicity）：一个原子事务要么完整执行，要么干脆不执行。这意味着，工作单元中的每项任务都必须正确执行。如果有任一任务执行失败，则整个工作单元或事务就会被终止。即此前对数据所作的任何修改都将被撤销。如果所有任务都被成功执行，事务就会被提交，即对数据所作的修改将会是永久性的。
2. 一致性（Consistency）：一致性代表了底层数据存储的完整性。它必须由事务系统和应用开发人员共同来保证。事务系统通过保证事务的原子性，隔离性和持久性来满足这一要求；应用开发人员则需要保证数据库有适当的约束（主键，引用完整性等），并且工作单元中所实现的业务逻辑不会导致数据的不一致，即，数据预期所表达的现实业务情况不相一致。例如，在一次转账过程中，从某一账户中扣除的金额必须与另一账户中存入的金额相等。支付宝账号100你读到余额要取，有人向你转100但是事物没提交（这时候你读到的余额应该是100，而不是200）这种就是一致性。
3. 隔离性（Isolation）：隔离性意味着事务必须在不干扰其他进程或事务的前提下独立执行。换言之，在事务或工作单元执行完毕之前，其所访问的数据不能受系统其他部分的影响。
4. 持久性（Durability）：持久性表示在某个事务的执行过程中，对数据所作的所有改动都必须在事务成功结束前保存至某种物理存储设备。这样可以保证，所作的修改在任何系统瘫痪时不至于丢失。

MyBatis

Mybait的优点：

1. 简单易学，容易上手（相比于Hibernate）基于SQL编程。
2. JDBC相比，减少了50%以上的代码量，消除了JDBC大量冗余的代码，不需要手动开关连接。
3. 很好的与各种数据库兼容（因为MyBatis使用JDBC来连接数据库，所以只要JDBC支持的数据库MyBatis都支持，而JDBC提供了可扩展性，所以只要这个数据库有针对Java的jar包就可以就可以与MyBatis兼容），开发人员不需要考虑数据库的差异性。
4. 提供了很多第三方插件，分页插件/逆向工程。
5. 能够与Spring很好的集成。
6. MyBatis相当灵活，不会对应用程序或者数据库的现有设计强加任何影响，SQL写在XML里，从程序代码中彻底分离，解除SQL与程序代码的耦合，便于统一管理和优化，并可重用。
7. 提供XML标签，支持编写动态SQL语句。
8. 提供映射标签，支持对象与数据库的ORM字段关系映射。
9. 提供对象关系映射标签，支持对象关系组建维护。

MyBatis框架的缺点

1. SQL语句的编写工作量较大，尤其是字段多、关联表多时，更是如此，对开发人员编写SQL语句的功底有一定要求。
2. SQL语句依赖于数据库，导致数据库移植性差，不能随意更换数据库。

+ #\{\}是预编译处理，\$\{\}是字符串替换。
+ Mybatis在处理#\{\}时，会将SQL中的#\{\}替换为?号，调用PreparedStatement的set方法来赋值。
+ Mybatis在处理\$\{\}时，就是把\$\{\}替换成变量的值。
+ 使用#0可以有效的防止SQL注入，提高系统安全性

Spring

是框架、容器、生态。

## BeanFactory和ApplicationContext

相同:

+ Spring提供了两种不同的IOC容器，一个是BeanFactory，另外一个是ApplicationContext，它们都是Javainterface，ApplicationContext继承于BeanFactory(ApplicationContext继承ListableBeanFactory。
+ 它们都可以用来配置XML属性，也支持属性的自动注入。
+ 而ListableBeanFactory继承BeanFactory，BeanFactory和ApplicationContext 都提供了一种方式，使用getBean("bean name")获取bean。

不同:

+ 当你调用getBean()方法时，BeanFactory仅实例化bean，而ApplicationContext 在启动容器的时候实例化单例bean，不会等待调用getBean()方法时再实例化。
+ BeanFactory不支持国际化，即i18n，但ApplicationContext提供了对它的支持。
+ BeanFactory与ApplicationContext之间的另一个区别是能够将事件发布到注册为监听器的bean。
+ BeanFactory的一个核心实现是XMLBeanFactory而ApplicationContext的一个核心实现是ClassPathXmIApplicationContext，Web容器的环境我们使用WebApplicationContext并且增加了getServletContext方法。
+ 如果使用自动注入并使用BeanFactory，则需要使用API注册AutoWiredBeanPostProcessor，如果使用ApplicationContext，则可以使用XML进行配置。
+ 简而言之，BeanFactory提供基本的IOC和DI功能，而ApplicationContext提供高级功能，BeanFactory可用于测试和非生产使用，但ApplicationContext是功能更丰富的容器实现，应该优于BeanFactory。

## SpringMVC工作流程

当发起请求时被前置的控制器拦截到请求，根据请求参数生成代理请求，找到请求对应的实际控制器，控制器处理请求，创建数据模型，访问数据库，将模型响应给中心控制器，控制器使用模型与视图渲染视图结果，将结果返回给中心控制器，再将结果返回给请求者。

1. DispatcherServlet表示前置控制器，是整个SpringMVC的控制中心。用户发出请求，DispatcherServlet接收请求并拦截请求。
2. HandlerMapping为处理器映射。DispatcherServlet调用HandlerMapping,.HandlerMapping根据请求url查找Handler。
3. 返回处理器执行链，根据url查找控制器，并且将解析后的信息传递给DispatcherServlet
4. HandlerAdapter表示处理器适配器，其按照特定的规则去执行Handler。
5. 执行handler找到具体的处理器
6. Controller将具体的执行信息返回给HandlerAdapter，如ModelAndView。
7. HandlerAdapter将视图逻辑名或模型传递给DispatcherServlet。
8. DispatcherServlet调用视图解析器(ViewResolver)来解析HandlerAdapter传递的逻辑视图名。
9. 视图解析器将解析的逻辑视图名传给DispatcherServlet。
10. DispatcherServlet根据视图解析器解析的视图结果，调用具体的视图，进行试图渲染11、将响应数据返回给客户端

## SpringMVC组件

1. HandlerMapping：根据request找到相应的处理器。因为Handler/Controller有两种形式，一种是基于类的Handler，另一种是基于Method的Handler （也就是我们常用的）。
2. HandlerAdapter：调用Handler的适配器。如果把Handler/Controller当做工具的话，那么HandlerAdapter就相当于干活的工人。
3. HandlerExceptionResolver：对异常的处理。
4. ViewResolver：用来将String类型的视图名和Locale解析为View类型的视图。
5. RequestToViewNameTranslator：有的Handler/Controller处理完后没有设置返回类型，比如是void方法，这是就需要从request中获取viewName
6. LocaleResolver：从request中解析出Locale。Locale表示一个区域，比如zh-cn，对不同的区域的用户，显示不同的结果，这就是i18n。SpringMVC中有具体的拦截器LocaleChangelnterceptor。
7. ThemeResolver：主题解析，这种类似于我们手机更换主题，不同的UI，CSS等。
8. MultipartResolver：处理上传请求，将普通的request封装成MultipartHttpServletRequest。
9. FlashMapManager：用于管理FlashMap，FlashMap用于在redirect重定向中传递参数。

## 事务传播机制

多个事务方法相互调用时，事务如何在这些方法之间进行传播，Spring中提供了7中不同的传播特性，来保证事务的正常执行：

+ REQUIRED：默认的传播特性，如果当前没有事务，则新建一个事务，如果当前存在事务，则加入这个事务。
+ SUPPORTS：当前存在事务，则加入当前事务，如果当前没有事务，则以非事务的方式执行。
+ MANDATORY：当前存在事务，则加入当前事务，如果当前事务不存在，则抛出异常。
+ REQUIRED_NEW：创建一个新事务，如果存在当前事务，则挂起改事务。
+ NOT_SUPPORTED：以非事务方式执行，如果存在当前事务，则挂起当前事务。
+ NEVER：不使用事务，如果当前事务存在，则抛出异常。
+ NESTED：如果当前事务存在，则在嵌套事务中执行，否则REQUIRED的操作一样。

NESTED和REQUIRED_NEW的区别：

REQUIRED_NEW是新建一个事务并且新开始的这个事务与原有事务无关，而NESTED则是当前存在事务时会开启一个嵌套事务，在NESTED情况下，父事务回滚时，子事务也会回滚，而REQUIRED_NEW情况下，原有事务回滚，不会影响新开启的事务。

NESTED和REQUIRED的区别:

REQUIRED情况下，调用方存在事务时，则被调用方和调用方使用同一个事务，那么被调用方出现异常时，由于共用一个事务，所以无论是否catch异常，事务都会回滚，而在NESTED情况下，被调用方发生异常时，调用方可以catch其异常，这样只有子事务回滚，父事务不会回滚。

## Spring设计模式

1. 工厂模式，在各种BeanFactory以及ApplicationContext创建中都用到。
2. 模版模式，在各种BeanFactory以及ApplicationContext实现中也都用到。
3. 代理模式，Spring AOP利用了AspectJ AOP实现的AspectJ AOP的底层用了动态代理。
4. 策略模式，加载资源文件的方式，使用了不同的方法，比如ClassPathResourece、FileSystemResource、ServletContextResource、UrlResource但他们都有共同的接口Resource。在AOP的实现中，采用了两种不同的方式，JDK动态代理和CGLIB代理。
5. 单例模式，比如在创建bean的时候。
6. 观察者模式,spring中的ApplicationEvent、ApplicationListener、ApplicationEventPublisher。
7. 适配器模式，MethodBeforeAdviceAdapter、ThrowsAdviceAdapter、AfterReturningAdapter。
8. 装饰者模式，源码中类型带Wrapper或者Decorator的都是。

## Spring事务原理

在使用Spring框架的时候，可以有两种事务的实现方式，一种是编程式事务，有用户自己通过代码来控制事务的处理逻辑，还有一种是声明式事务，通过@Transactional注解来实现。

其实事务的操作本来应该是由数据库来进行控制，但是为了方便用户进行业务逻辑的操作，spring对事务功能进行了扩展实现，一般我们很少会用编程式事务，更多的是通过添加@Transactional注解来进行实现，当添加此注解之后事务的自动功能就会关闭，有spring框架来帮助进行控制。

其实事务操作是AOP的一个核心体现，当一个方法添加@Transactional注解之后，Spring会基于这个类生成一个代理对象，会将这个代理对象作为bean，当使用这个代理对象的方法的时候，如果有事务处理，那么会先把事务的自动提交给关系，然后去执行具体的业务逻辑，如果执行逻辑没有出现异常，那么代理逻辑就会直接提交，如果出现任何异常情况，那么直接进行回滚操作，当然用户可以控制对哪些异常进行回滚操作。

## Spring事务失效

1. bean对象没有被spring容器管理。
2. 方法的访问修饰符不是public。
3. 自身调用问题。
4. 数据源没有配置事务管理器。
5. 数据库不支持事务。
6. 异常被捕获。
7. 异常类型错误或者配置错误。

## Spring简化开发

+ 基于POJO的轻量级和最小侵入性编程。
+ 通过依赖注入和面向接口实现松耦合。
+ 基于切面和惯例进行声明式编程。
+ 通过切面和模板减少样板式代码。

## Spring支持的bean作用域有哪些

+ singleton：使用该属性定义Bean时，IOC容器仅创建一个ean实例，IOC容器每次返回的是同一个Bean实例。
+ prototype：使用该属性定义Bean时，IOC容器可以创建多个Bean实例，每次返回的都是一个新的实例。
+ request：该属性仅对HTTP请求产生作用，使用该属性定义Bean时，每次HTTP请求都会创建一个新的Bean，适用于WebApplicationContext环境。
+ session：该属性仅用于HTTP Session，同一个Session共享一个Bean实例。不同Session使用不同的实例。
+ global-session：该属性仅用于HTTP Session，同session作用域不同的是，所有的Session共享一个Bean实例。

## Bean生命周期

1. 实例化bean对象。通过反射的方式进行对象的创建，此时的创建只是在堆空间中申请空间，属性都是默认值。
2. 设置对象属性，给对象中的属性进行值的设置工作。
3. 检查Aware相关接口并设置相关依赖。如果对象中需要引用容器内部的对象，那么需要调用aware接口的子类方法来进行统一的设置。
4. BeanPostProcessor的前置处理。对生成的bean对象进行前置的处理工作。
5. 检查是否是InitializingBean的子类来决定是否调用afterPropertiesSet方法。判断当前bean对象是否设置了InitializingBean接口，然后进行属性的设置等基本工作。
6. 检查是否配置有自定义的init-method方法。如果当前bean对象定义了初始化方法，那么在此处调用初始化方法。
7. BeanPostProcessor后置处理。对生成的bean对象进行后置的处理工作。
8. 注册必要的Destruction相关回调接口。为了方便对象的销毁，在此处调用注销的回调接口，方便对象进行销毁操作。

## 自动装配

bean的自动装配指的是bean的属性值在进行注入的时候通过某种特定的规则和方式去容器中查找，并设置到具体的对象属性中，主要有五种方式：

1. no：缺省情况下，自动配置是通过"ref"属性手动设定，在项目中最常用。
2. byName：根据属性名称自动装配。如果一个bean的名称和其他bean属性的名称是一样的，将会自装配它。
3. byType：按数据类型自动装配，如果bean的数据类型是用其它bean属性的数据类型，兼容并自动装配它。
4. constructor：在构造函数参数的byType方式。
5. autodetect：如果找到默认的构造函数，使用“自动装配用构造"；否则，使用“按类型自动装配"。

## Spring优势

1. Spring通过DI和AOP和消除样板式代码来简化企业级Java开发。
2. Spring框架之外还存在一个构建在核心框架之上的庞大生态圈，它将Spring扩展到不同的领域，如Web服务，REST，移动开发以及NoSQL。
3. 低侵入式设计，代码的污染极低。
4. 独立于各种应用服务器，基于Spring框架的应用，可以真正实现Write Once,Run Anywhere的承诺。
5. Spring的loC容器降低了业务对象替换的复杂性，提高了组件之间的解耦。
6. Spring的AoP支持允许将一些通用任务如安全，事务，日志等进行集中式处理，从而提供了更好的复用。
7. Spring的ORM和DAO提供了与第三方持久层框架的的良好整合，并简化了底层的数据库访问。
8. Spring的高度开放性，并不强制应用完全依赖于Spring，开发者可自由选用Spring框架的部分或全部。

## AOP

AOP全称叫做Aspect Oriented Programming面向切面编程。它是为解耦而生的，解耦是程序员编码开发过程中一直追求的境界，AOP在业务类的隔离上，绝对是做到了解耦，在这里面有几个核心的概念：

+ 切面（Aspect）：指关注点模块化，这个关注点可能会横切多个对象。事务管理是企业级Java应用中有关横切关注点的例子。在Spring AOP中，切面可以使用通用类基于模式的方式（schema-based approach）或者在普通类中以@Aspect注解（@AspectJ注解方式）来实现。
+ 连接点（Join point）：在程序执行过程中某个特定的点，例如某个方法调用的时间点或者处理异常的时间点。在Spring AOP中，一个连接点总是代表一个方法的执行。
+ 通知（Advice）：在切面的某个特定的连接点上执行的动作。通知有多种类型，包括"around" , "before" and“after"等等。通知的类型将在后面的章节进行讨论。许多AOP框架，包括Spring在内，都是以拦截器做通知模型的，并维护着一个以连接点为中心的拦截器链。
切点 （Pointcut）：匹配连接点的断言。通知和切点表达式相关联，并在满足这个切点的连接点上运行（例如，当执行某个特定名称的方法时）。切点表达式如何和连接点匹配是AOP的核心： Spring默认使用Aspect）切点语义。
+ 引入 （Introduction）：声明额外的方法或者某个类型的字段。Spring允许引入新的接口（以及一个对应的实现）到任何被通知的对象上。例如，可以使用引入来使bean实现工sModified接口，以便简化缓存机制（在AspectJ社区，引入也被称为内部类型声明inter）。
+ 目标对象（Target object）：被一个或者多个切面所通知的对象。也被称作被通知（advised）对象。既然Spring AOP是通过运行时代理实现的，那么这个对象永远是一个被代理（proxied）的对象。
+ AOP代理（AOP proxy） ：AOP框架创建的对象，用来实现切面契约（aspect contract）（包括通知方法执行等功能）。在Spring中，AOP代理可以是IDK动态代理或CGLIB代理。
+ 织入(Weaving):把切面连接到其它的应用程序类型或者对象上，并创建一个被被通知的对象的过程。这个过程可以在编译时（例如使用Aspect]编译器)、类加载时或运行时中完成。Spring和其他纯Java AOP框架一样，是在运行时完成织入的。

任何一个系统都是由不同的组件组成的，每个组件负责一块特定的功能，当然会存在很多组件是跟业务无关的，例如日志、事务、权限等核心服务组件，这些核心服务组件经常融入到具体的业务逻辑中，如果我们为每一个具体业务逻辑操作都添加通用的代码，很明显代码冗余太多，因此我们需要将这些公共的代码逻辑抽象出来变成一个切面，然后注入到目标对象（具体业务）中去，AOP正是基于这样的一个思路实现的，通过动态代理的方式，将需要注入切面的对象进行代理，在进行调用的时候，将公共的逻辑直接添加进去，而不需要修改原有业务的逻辑代码，只需要在原来的业务逻辑基础之上做一些增强功能即可。

## IoC

SpringBoot

## Starter

使用spring+springmvc框架进行开发的时候，如果需要引入mybatis框架，那么需要在xml中定义需要的bean对象，这个过程很明显是很麻烦的，如果需要引入额外的其他组件，那么也需要进行复杂的配置，因此在springboot中引入了starter。

starter就是一个jar包，写一个@Configuration的配置类，将这些bean定义在其中，然后再starter包的META-INF/spring.factories中写入配置类，那么springboot程序在启动的时候就会按照约定来加载该配置类。

开发人员只需要将相应的starter包依赖进应用中，进行相关的属性配置，就可以进行代码开发，而不需要单独进行bean对象的配置。

## 嵌入式服务器

在springboot框架中，大家应该发现了有一个内嵌的tomcat，在之前的开发流程中，每次写好代码之后必须要将项目部署到一个额外的web服务器中，只有这样才可以运行，这个明显要麻烦很多，而使用springboot的时候，你会发现在启动项目的时候可以直接按照java应用程序的方式来启动项目，不需要额外的环境支持，也不需要tomcat服务器，这是因为在springboot框架中内置了tomcat.jar，来通过main方法启动容器，达到一键开发部署的方式，不需要额外的任何其他操作。

分布式

## 分布式ID生成

名称|描述|优点|缺点
:--:|:--:|:--:|:--:
UUID|UUID是通用唯—标识码的缩写，其目的是让分布式系统中的所有元素都有唯一的辨识信息，而不需要通过中央控制器来指定唯—标识|1.降低全局节点的压力，使得主键生成速度更快；2.生成的主键全局唯一；3.跨服务器合并数据方便|1.UUID占用16个字符，空间占用较多；2.不是递增有序的数字，数据写入IO随机性很大，且索引效率下降
数据库主键自增|MySQL数据库设置主键且主键自动增长|1.INT和BIGINT类型占用空间较小；2.主键自动增长，IO写入连续性好；3.数字类型查询速度优于字符串|1.并发性能不高，受限于数据库性能；2.分库分表，需要改造，复杂；3.自增:数据和数据量泄露
Redis自增|Redis计数器，原子性自增|使用内存，并发性能好|1.数据丢失；2.自增导致数据量泄露
雪花算法|snowflake，经典解决方案|1.不依赖外部组件；2.性能好|时钟回拨

雪花算法ID分布：

1. 符号位，占用1位。
2. 时间戳，占用41位，可以支持69年的时间跨度。
3. 机器ID，占用10位。
4. 序列号，占用12位。一毫秒可以生成4095个ID。

## 分布式锁应用场景

1. 系统是一个分布式系统,集群集群，java的锁已经锁不住了。
2. 作共享资源，比如库里唯一的用户数据。
3. 同步访问，即多个进程同时操作共享资源。

## 分布式锁生成

1. Redis的分布式锁，很多大公司会基于Redis做扩展开发。setnx，Redisson。
2. 基于Zookeeper。顺序临时节点。
3. 基于数据库，比如Mysql。主键或唯一索引的唯一性。

## Redis分布式锁实现

假设有两个服务A、B都希望获得锁，执行过程大致如下：

1. 服务A为了获得锁，向Redis发起如下命令: SET productld:lock Oxx9p030q/1 NXEX 30000其中,"productld"由自己定义，可以是与本次业务有关的id，"Oxx9p03001"是一串随机值，必须保证全局唯一，“NX"指的是当且仅当key(也就是案例中的"productld:lock")在Redis中不存在时，返回执行成功，否则执行失败。"EX 30000"指的是在30秒后，key将被自动删除。I执行命令后返回成功，表明服务成功的获得了锁。
2. 服务B为了获得锁，向Redis发起同样的命令:SET productld:lock 0000111 NX EX 30000。由于Redis内已经存在同名key，且并未过期，因此命令执行失败，服务B未能获得锁。服务B进入循环请求状态，比如每隔1秒钟(自行设置)向Redis发送请求，直到执行成功并获得锁。
3. 服务A的业务代码执行时长超过了30秒，导致key超时，因此Redis自动删除了key。此时服务B再次发送命令执行成功，假设本次请求中设置的value值为0000222。此时需要在服务A中对key进行续期（watch dog）。
4. 服务A执行完毕，为了释放锁，服务A会主动向Redis发起删除key的请求。注意:在删除key之前，一定要判断服务A持有的value与Redis内存储的value是否一致。比如当前场景下，Redis中的锁早就不是服务A持有的那一把了，而是由服务2创建，如果贸然使用服务A持有的key来删除锁，则会误将服务2的锁释放掉。此外，由于删除锁时涉及到一系列判断逻辑，因此一般使用lua脚本，具体如下:

```lua
if redis.call("get"， KEYS[1])==ARGV[1] then
    return redis.call("del"，KEYS[1])
else
    return 0
end
```

## Redis分布式锁造成死锁

1. 加锁，没有释放锁。需要加释放锁的操作。比如delete key。
2. 加锁后，程序还没有执行释放锁，程序挂了。需要用的key的过期机制。

## ZooKeeper分布式锁实现

顺序节点特性：使用ZooKeeper的顺序节点特性，假如我们在/lock/目录下创建3个节点，ZK集群会按照发起创建的顺序来创建节点，节点分别为/lock/0000000001、/lock/0000000002、/lock/0000000003，最后一位数是依次递增的，节点名由zk来完成。

临时节点特性：ZK中还有一种名为临时节点的节点，临时节点由某个客户端创建，当客户端与ZK集群断开连接，则该节点自动被删除。EPHEMERAL_SEQUENTIAL为临时顺序节点。

根据ZK中节点是否存在，可以作为分布式锁的锁状态，以此来实现一个分布式锁，下面是分布式锁的基本逻辑：

1. 客户端1调用create()方法创建名为"/业务ID/lock-"的临时顺序节点。
2. 客户端1调用getChildren(“业务ID)方法来获取所有已经创建的子节点。
3. 客户端获取到所有子节点path之后，如果发现自己在步骤1中创建的节点是所有节点中序号最小的，就是看自己创建的序列号是否排第一，如果是第一，那么就认为这个客户端1获得了锁，在它前面没有别的客户端拿到锁。
4. 如果创建的节点不是所有节点中需要最小的，那么则监视比自己创建节点的序列号小的最大的节点，进入等待。直到下次监视的子节点变更的时候，再进行子节点的获取，判断是否获取锁。

## Redis和ZooKeeper分布式锁实现区别

Redis：

1. Redis只保证最终一致性，副本间的数据复制是异步进行（Set是写，Get是读，Redis集群一般是读写分离架构，存在主从同步延迟情况)，主从切换之后可能有部分数据没有复制过去可能会「丢失锁」情况，故强一致性要求的业务不推荐使用Redis，推荐使用zk。
2. Redis集群各方法的响应时间均为最低。随着并发量和业务数量的提升其响应时间会有明显上升(公网集群影响因素偏大)，但是极限qps可以达到最大且基本无异常。

ZooKeeper：

1. 使用ZooKeeper集群，锁原理是使用ZooKeeper的临时顺序节点，临时顺序节点的生命周期在Client与集群的Session结束时结束。因此如果某个Client节点存在网络问题，与ZooKeeper集群断开连接，Session超时同样会导致锁被错误的释放（导致被其他线程错误地持有)，因此ZooKeeper也无法保证完全一致。
2. ZK具有较好的稳定性;响应时间抖动很小，没有出现异常。但是随着并发量和业务数量的提升其响应时间和qps会明显下降。

总结:

1. Zookeeper每次进行锁操作前都要创建若干节点，完成后要释放节点，会浪费很多时间。
2. 而Redis只是简单的数据操作，没有这个问题。

## MySQL分布式锁实现

在Mysql中创建一张表，设置一个主键或者UNIQUE KEY这个KEY就是要锁的KEY，所以同一个KEY在mysql表里只能插入一次了，这样对锁的竞争就交给了数据库，处理同一个KEY数据库保证了只有一个节点能插入成功，其他节点都会插入失败。

DB分布式锁的实现:通过主键id或者唯一索性的唯一性进行加锁，说白了就是加锁的形式是向一张表中插入一条数据，该条数据的id就是一把分布式锁，例如当一次请求插入了一条id为1的数据，其他想要进行插入数据的并发请求必须等第一次请求执行完成后删除这条id为1的数据才能继续插入，实现了分布式锁的功能。

这样lock和unlock的思路就很简单了，伪代码：

```txt
def lock :
    exec sql: insert into locked-table (xxx) values (xxx)
    if result == true :
        return true
    else :
        return false
def unlock :
    exec sq1: delete from lockedorder where order_id='order_id'
```

## 计算器算法

计数器算法，是指在指定的时间周期内累加访问次数，达到设定的阈值时，触发限流策略。下一个时间周期进行访问时，访问次数清零。此算法无论在单机还是分布式环境下实现都非常简单，使用redis的incr原子自增性，再结合key的过期时间，即可轻松实现。

## 滑动时间窗口算法

为了解决计数器算法的临界值的问题，发明了滑动窗口算法。在TCP网络通信协议中，就采用滑动时间窗口算法来解决网络拥堵问题。

滑动时间窗口是将计数器算法中的实际周期切分成多个小的时间窗口，分别在每个小的时间窗口中记录访问次数，然后根据时间将窗口往前滑动并删除过期的小时间窗口。最终只需要统计滑动窗口范围内的小时间窗口的总的请求数即可。

## 漏桶限流算法

漏桶算法的原理就像它的名字一样，我们维持一个漏斗，它有恒定的流出速度，不管水流流入的速度有多快，漏斗出水的速度始终保持不变，类似于消息中间件，不管消息的生产者请求量有多大，消息的处理能力取决于消费者。
漏桶的容量=漏桶的流出速度*可接受的等待时长。在这个容量范围内的请求可以排队等待系统的处理，超过这个容量的请求，才会被抛弃。

在漏桶限流算法中，存在下面几种情况：

1. 当请求速度大于漏桶的流出速度时，也就是请求量大于当前服务所能处理的最大极限值时，触发限流策略
2. 请求速度小于或等于漏桶的流出速度时，也就是服务的处理能力大于或等于请求量时，正常执行。

漏桶算法有一个缺点：当系统在短时间内有突发的大流量时，漏桶算法处理不了。

## 令牌桶限流算法

令牌桶算法，是增加一个大小圉定的容器，也就是令牌桶，系统以恒定的速率向令牌桶中放入令牌，如果有客户端来请求，先需要从令牌桶中拿一个令牌，拿到令牌，才有资格访问系统，这时令牌桶中少一个令牌。当令牌桶满的时候，再向令牌桶生成令牌时，令牌会被抛弃。

在令牌桶算法中，存在以下几种情况：

1. 请求速度大于令牌的生成速度︰那么令牌桶中的令牌会被取完，后续再进来的请求，由于拿不到令牌，会被限流。
2. 请求速度等于令牌的生成速度：那么此时系统处于平稳状态。
3. 请求速度小于令牌的生成速度：那么此时系统的访问星沅远低于系统的并发能力，请求可以被正常处理。

令牌桶算法，由于有一个桶的存在，可以处理短时间大流量的场景，如果流量数不大于令牌数。这是令牌桶和漏桶的一个区别。

## 微服务设计原则

1. 单一职责原则：让每个服务能独立，有界限的工作，每个服务只关注自己的业务。做到高内聚。
2. 服务自治原则：每个服务要能做到独立开发、独立测试、独立构建、独立部署，独立运行。与其他服务进行解耦。
3. 轻星级通信原则：让每个服务之间的调用是轻星级，并且能够跨平台、跨语言。比如采用RESTful风格，利用消息队列进行通信等。
4. 粒度进化原则：对每个服务的粒度把控，其实没有统一的标准，这个得结合我们解决的具体业务问题。不要过度设计。服务的粒度随着业务和用户的发展而发展。

总结一句话，软件是为业务服务的，好的系统不是设计出来的，而是进化出来的。

## 分布式系统谬论

+ 网络相当可靠。
+ 延迟为零。
+ 传输带宽是无限的。
+ 网络相当安全。
+ 拓扑结构不会改变。
+ 必须要有一名管理员。
+ 传输成本为零。
+ 网络同质化。

## BASE理论

由于CAP中一致性C和可用性A无法兼得，eBay的架构师，提出了BASE理论，它是通过牺牲数据的强一致性，来获得可用性。它由于如下3种特征：

+ Basically Available (基本可用)︰分布式系统在出现不可预知故障的时候，允许损失部分可用性，保证核心功能的可用。
+ Soft state（软状态)∶软状态也称为弱状态，和硬状态相对，是指允许系统中的数据存在中间状态，并认为该中间状态的存在不会影响系统的整体可用性，即允许系统在不同节点的数据副本之间进行数据同步的过程存在延时。
+ Eventually consistent (最终一致性)∶最终一致性强调的是系统中所有的数据副本，在经过一段时间的同步后，最终能够达到一个—致的状态。因此，最终一致性的本质是需要系统保证最终数据能够达到一致，而不需要实时保证系统数据的强一致性。

BASE理论并没有要求数据的强一致性，而是允许数据在一定的时间段内是不一致的，但在最终某个状态会达到一致。在生产环境中，很多公司，会采用BASE理论来实现数据的一致，因为产品的可用性相比强—致性来说，更加重要。比如在电商平台中，当用户对一个订单发起支付时，往往会调用第三方支付平台，比如支付宝支付或者微信支付，调用第三方成功后，第三方并不能及时通知我方系统，在第三方没有通知我方系统的这段时间内，我们给用户的订单状态显示支付中，等到第三方回调之后，我们再将状态改成已支付。虽然订单状态在短期内存在不一致，但是用户却获得了更好的产品体验。

## 2PC和3PC的区别

1. 3pct比2pc多了一个can commit阶段，减少了不必要的资源浪费。因为2pc在第一阶段会占用资源，而3pc在这个阶段不占用资源，只是校验—下sql，如果不能执行，就直接返回，减少了资源占用。
2. 引入超时机制。同时在协调者和参与者中都引入超时机制。

+ 2pc：只有协调者有超时机制，超时后，发送回滚指令。
+ 3pc：协调者和参与者都有超时机制。
  + 协调者超时：CanCommit、PreCommit阶段，发送中断指令。
  + 参与者超时：PreCommit阶段进行中断，DoCommit阶段进行提交。

## 幂等处理

即对于同样的消息传递都要进行同样的处理，即解决重复消息的问题

1. 查询操作：查询一次和查询多次，在数据不变的情况下，查询结果是一样的。select是天然的幂等操作;
2. 删除操作：删除操作也是幂等的，删除一次和多次删除都是把数据删除。(注意可能返回结果不一样，删除的数据不存在，返回0，删除的数据多条，返回结果多个。
3. 唯一索引：防止新增脏数据。比如:支付宝的资金账户，支付宝也有用户账户，每个用户只能有一个资金账户，怎么防止给用户创建多个资金账户，那么给资金账户表中的用户ID加唯一索引，所以一个用户新增成功一个资金账户记录。要点：唯一索引或唯一组合索引来防止新增数据存在脏数据（当表存在唯一索引，并发时新增报错时，再查询一次就可以了，数据应该已经存在了，返回结果即可。
4. token机制：防止页面重复提交。

+ 业务要求：页面的数据只能被点击提交一次。
+ 发生原因：由于重复点击或者网络重发，或者nginx重发等情况会导致数据被重复提交。
+ 解决办法：
  + 集群环境采用token加redis(redis单线程的，处理需要排队)。
  + 单JVM环境：采用token加redis或token 加jvm锁。
+ 处理流程：
  + 数据提交前要向服务的申请token，token放到redis或jvm内存，token有效时间。

## TCC解决方案

Tcc (TrnfConfirm-Cancel)是一种常用的分布式事务解决方案，它将一个事务拆分成三个步骤：

+ T(Try)：业务检查阶段，这阶段主要进行业务校验和检查或者资源预留;也可能是直接进行业务操作。
+ C(Confirm)：业务确认阶段，这阶段对Try阶段校验过的业务或者预留的资源进行确认。
+ C(Cancel)：业务回滚阶段，这阶段和上面的C(Confirm)是互斥的，用于释放Try阶段预留的资源或者业务。

## TCC空回滚

在没有调用TCC资源Try方法的情况下，调用了二阶段的Cancel方法。比如当Try请求由于网络延迟或故障等原因，没有执行，结果返回了异常，那么此时Cancel就不能正常执行，因为Try没有对数据进行修改，如果Cancel进行了对数据的修改，那就会导致数据不一致。

解决思路是关键就是要识别出这个空回滚。思路很简单就是需要知道Try阶段是否执行，如果执行了，那就是正常回滚;如果没执行，那就是空回滚。建议TM在发起全局事务时生成全局事务记录，全局事务ID贯穿整个分布式事务调用链条。再额外增加一张分支事务记录表，其中有全局事务ID和分支事务ID，第一阶段Try方法里会插入一条记录，表示Try阶段执行了。Cancel接口里读取该记录，如果该记录存在，则正常回滚;如果该记录不存在，则是空回滚。

## TCC幂等

为了保证TCC二阶段提交重试机制不会引发数据不一致，要求TCC的二阶段Confirm和Cancel接口保证幂等，这样不会重复使用或者释放资源。如果幂等控制没有做好，很有可能导致数据不一致等严重问题。

解决思路在上述分支事务记录中增加执行状态，每次执行前都查询该状态。

## TCC悬挂

悬挂就是对于一个分布式事务，其二阶段Cancel接口比Try接口先执行。

出现原因是在调用分支事务Try时，由于网络发生拥堵，造成了超时，TM就会通知RM回滚该分布式事务，可能回滚完成后，Try请求才到达参与者真正执行，而一个Try方法预留的业务资源，只有该分布式事务才能使用，该分布式事务第一阶段预留的业务资源就再也没有人能够处理了，对于这种情况，我们就称为悬挂，即业务资源预留后无法继续处理。

解决思路是如果二阶段执行完成，那一阶段就不能再继续执行。在执行一阶段事务时判断在该全局事务下，判断分支事务记录表中是否已经有二阶段事务记录，如果有则不执行Try.

## 可靠消息服务方案

可靠消息最终一致性方案指的是:当事务的发起方（事务参与者，消息发送者）执行完本地事务后，同时发出—条消息，事务参与方（事务参与者，消息的消费者)一定能够接受消息并可以成功处理自己的事务。

这里面强调两点：

1. 可靠消息:发起方—定得把消息传递到消费者。
2. 最终一致性:最终发起方的业务处理和消费方的业务处理得完成，达成最终一致。

## 最大努力通知方案的关键

1. 有一定的消息重复通知机制。因为接收通知方可能没有接收到通知，此时要有一定的机制对消息重复通知。
2. 消息校对机制。如果尽最大努力也没有通知到接收方，或者接收方消费消息后要再次消费，此时可由接收方主动向通知方查询消息信息来满足需求。

## 对外提供的API保证幂等

举例说明：银联提供的付款接口，需要接入商户提交付款请求时附带，source来源，seq序列号。

source+seq在数据库里面做唯一索引，防止多次付款(并发时，只能处理一个请求)。

重点：对外提供接口为了支持幂等调用，接口有两个字段必须传，一个是来源source，一个是来源方序列号seq，这个两个字段在提供方系统里面做联合唯一索引，这样当第三方调用时，先在本方系统里面查询一下，是否已经处理过，返回相应处理结果;没有处理过，进行相应处理，返回结果。

注意，为了幕等友好，一定要先查询一下，是否处理过该笔业务，不查询直接插入业务系统，会报错，但实际已经处理。

## 双写一致性问题解决

先做一个说明，从理论上来说，给缓存设置过期时间，是保证最终一致性的解决方案。这种方案下，我们可以对存入缓存的数据设置过期时间，所有的写操作以数据库为准，对缓存操作只是尽最大努力更新即可。也就是说如果数据库写成功，缓存更新失败，那么只要到达过期时间，则后面的读请求自然会从数据库中读取新值然后回填缓存。因此，接下来讨论的思路不依赖于给缓存设置过期时间这个方案。

大前提：

先读缓存，如果缓存没有，才从数据库读取。

在这里，我们讨论三种更新策略：

1. 先更新缓存，再更新数据库。不可用。因为线程不安全，且浪费性能。
2. 先更新数据库，再更新缓存。不可用。未来保证一致性需要使用延时双删策略。
3. 先删除缓存，再更新数据库。不可用。未来保证一致性需要使用延时双删策略或Canul监听消息队列。
4. 先更新数据库，再删除缓存。可用，但存在问题。

## 认证和授权

Authentication (认证)是验证您的身份的凭据（例如用户名/用户ID和密码)，通过这个凭据，系统得以知道你就是你，也就是说系统存在你这个用户。所以，Authentication被称为身份/用户验证。

Authorization(授权）发生在Authentication (认证)之后。授权，它主要掌管我们访问系统的权限。比如有些特定资源只能具有特定权限的人才能访问比如admin，有些对系统资源操作比如删除、添加、更新只能特定人才具有。
这两个一般在我们的系统中被结合在一起使用，目的就是为了保护我们系统的安全性。

## Cookie和Session

Session的主要作用就是通过服务端记录用户的状态。

Cookie数据保存在客户端(浏览器端)，Session数据保存在服务器端。相对来说Session安全性更高。如果使用Cookie的话，一些敏感信息不要写入Cookie中，最好能将Cookie信息加密然后使用到的时候再去服务器端解密。

那么，如何使用Session进行身份验证

很多时候我们都是通过SessionID来指定特定的用户，SessionID一般会选择存放在服务端。举个例子∶用户成功登陆系统，然后返回给客户端具有SessionID的Cookie，当用户向后端发起请求的时候会把 SessionID带上，这样后端就知道你的身份状态了。关于这种认证方式更详细的过程如下:

+ 用户向服务器发送用户名和密码用于登陆系统。
+ 服务器验证通过后，服务器为用户创建一个Session，并将Session信息存储起来。服务器向用户返回—个SessionID，写入用户的Cookie。
+ 当用户保持登录状态时，Cookie将与每个后续请求—起被发送出去。
+ 服务器可以将存储在Cookie 上的Session lD与存储在内存中或者数据库中的Session信息进行比较，以验证用户的身份，返回给用户客户端响应信息的时候会附带用户当前的状态。

## JWT与Token

我们知道Session信息需要保存一份在服务器端。这种方式会带来一些麻烦，比如需要我们保证保存Session信息服务器的可用性、不适合移动端(不依赖Cookie)等。

有没有一种不需要自己存放Session信息就能实现身份验证的方式呢?使用Token即可! JWT (JSON webToken）就是这种方式的实现，通过这种方式服务器端就不需要保存Session数据了，只用在客户端保存服务端返回给客户的Token就可以了，扩展性得到提升。

JWT本质上就一段签名的JSON格式的数据。由于它是带有签名的，因此接收者便可以验证它的真实性。下面是 RFC 7519对JWT做的较为正式的定义。

JSON Web Token(WT) is a compact, URL-safe means of representing claims to be transferred between twoparties. The claims in a JWT are encoded as a JSON object that is used as the payload of a ]SON web
Signature (WS) structure or as the plaintext of a ]SON Web Encryption(WE) structure,enabling the claimsto be digitally signed or integrity protected with a Message Authentication Code (MAC) and/or encrypted.——JSON Web Token (JWT)

JWT由3部分构成：

+ Header :描述JWT的元数据。定义了生成签名的算法以及Token的类型。
+ Payload(负载):用来存放实际需要传递的数据。
+ Signature(签名)︰服务器通过Payload、Header和一个密钥(secret)使用Header里面指定的签名算法（默认是HMAC SHA256)生成。

在基于Token进行身份验证的的应用程序中，服务器通过Payload、Header和一个密钥(secret)创建令牌(Token)并将Token 发送给客户端，客户端将Token保存在Cookie或者localStorage里面，以后客户端发出的所有请求都会携带这个令牌。你可以把它放在Cookie里面自动发送，但是这样不能跨域，所以更好的做法是放在HTTP Header的Authorization字段中:Authorization: Bearer Token。

+ 用户向服务器发送用户名和密码用于登陆系统。
+ 身份验证服务响应并返回了签名的JWT，上面包含了用户是谁的内容。
+ 用户以后每次向后端发请求都在Header中带上JWT。
+ 服务端检查JWT并从中获取用户相关信息。

## 为什么Cookie无法防止CSRF攻击，而token可以

CSRF (Cross Site Request Forgery)一般被翻译为跨站请求伪造。那么什么是跨站请求伪造呢?说简单一点用你的身份去发送—些对你不友好的请求。举个简单的例子：

小壮登录了某网上银行，他来到了网上银行的帖子区，看到一个帖子下面有一个链接写着"科学理财，年收益率70%”，小壮好奇的点开了这个链接，结果发现自己的账户少了10000元。这是这么回事呢?原来黑客在链接中藏了一个请求，这个请求直接利用小壮的身份给银行发送了一个转账请求,也就是通过你的Cookie向银行发出请求。\<a src=http:/www.mybank.com/Transfer?bankld=11&money=10000\>科学理财，年收益率70%</>，原因是进行Session认证的时候，我们一般使用Cookie 来存储Sessionld,当我们登陆后后端生成一个Sessionld放在Cookie中返回给客户端，服务端通过Redis或者其他存储工具记录保存着这个Sessionid，客户端登录以后每次请求都会带上这个Sessionld，服务端通过这个Sessionld来标示你这个人。如果别人通过cookie拿到了Sessionld后就可以代替你的身份访问系统了。

Session认证中Cookie 中的Sessionld是由浏览器发送到服务端的，借助这个特性，攻击者就可以通过让用户误点攻击链接，达到攻击效果。

但是，我们使用token 的话就不会存在这个问题，在我们登录成功获得token之后，一般会选择存放在localstorage 中。然后我们在前端通过某些方式会给每个发到后端的请求加上这个token，这样就不会出现CSRF漏洞的问题。因为，即使有个你点击了非法链接发送了请求到服务端，这个非法请求是不会携带token的，所以这个请求将是非法的。

## Session共享

1. 不要有session:但是确实在某些场景下，是可以没有session的，其实在很多接口类系统当中，都提倡【API无状态服务】﹔也就是每一次的接口访问，都不依赖于session、不依赖于前一次的接口访问，用jwt的token。
2. 存入cookie中:将session存储到cookie中，但是缺点也很明显，例如每次请求都得带着session，数据存储在客户端本地，是有风险的。
3. session同步:对个服务器之间同步session，这样可以保证每个服务器上都有全部的session信息，不过当服务器数呈比较多的时候，同步是会有延迟甚至同步失败。
4. 我们现在的系统会把session放到Redis中存储，虽然架构上变得复杂，并且需要多访问一次Redis，但是这种方案带来的好处也是很大的:实现session共享，可以水平扩展（增加Redis服务器)，服务器重启session不丢失(不过也要注意session在Redis中的刷新/失效机制)，不仅可以跨服务器session共享，甚至可以跨平台(例如网页端和APP端)进行共享。
5. 使用Nginx(或其他复杂均衡软硬件)中的ip绑定策略，同一个lp只能在指定的同一个机器访问，但是这样做风险也比较大，而且也是去了负载均衡的意义;

## 注册中心原理

服务启动后向Eureka注册，Eureka Server会将注册信息向其他Eureka Server进行同步，当服务消费者要调用服务提供者，则向服务注册中心获取服务提供者地址，然后会将服务提供者地址缓存在本地，下次再调用时，则直接从本地缓存中获取服务列表来完成服务调用。

## 配置中心原理

在服务运行之前，将所需的配置信息从配置仓库拉取到本地服务，达到统一化配置管理的目的。

配置中心是如何实现自动刷新的：

1. 配置中心Server端承担起配置刷亲新的职责。
2. 提交配置触发post请求给server端的bus/refresh接口。
3. server端接收到请求并发送给Spring Cloud Bus总线。
4. Spring Cloud bus接到消息并通知给其它连接到总线的客户端。
5. 其它客户端接收到通知，请求Server端获取最新配置。
6. 全部客户端均获取到最新的配置

## 配置中心保证数据安全

1. 保证容器文件访问的安全性，即保证所有的网络资源请求都需要登录。
2. 将配置中心里所有配置文件中的密码进行加密，保证其密文性。
3. 开发环境禁止拉取生产环境的配置文件。

## SpringCloud和Dubbo

1. dubbo是二进制传输。对象直接转成二进制，使用RPC通信。SpringCloud是http 传输，同时使用http协议一般会使用JSON报文，JSON转二进制，消耗会更大。
2. Dubbo只是实现了服务治理，而Spring Cloud下面有几十个子项目分别覆盖了微服务架构下的方方面面，服务治理只是其中的一个方面，一定程魔来说，Dubbo只是Spring Cloud Netflikx中的一个子集。
